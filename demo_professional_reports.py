#!/usr/bin/env python3
"""
Demonstration script for Professional Report Generation
Shows how to use the enhanced reporting features of the LLM Evaluation Framework.
"""

import os
import sys
from llm_eval.reporting.professional_report_generator import generate_professional_report_from_json

def demo_professional_reports():
    """Demonstrate the professional report generation features."""
    
    print("ğŸš€ LLM Evaluation Framework - Professional Reports Demo")
    print("=" * 60)
    
    # Check for existing results files
    results_dir = "results"
    if not os.path.exists(results_dir):
        print("âŒ No results directory found. Please run an evaluation first.")
        print("\nExample command:")
        print("python llm_eval_cli.py --model hf --model_name microsoft/DialoGPT-medium --tasks arc_easy --num_samples 5")
        return
    
    # Find JSON results files
    json_files = [f for f in os.listdir(results_dir) if f.endswith('.json')]
    
    if not json_files:
        print("âŒ No JSON results files found. Please run an evaluation first.")
        print("\nExample command:")
        print("python llm_eval_cli.py --model hf --model_name microsoft/DialoGPT-medium --tasks arc_easy --num_samples 5")
        return
    
    print(f"ğŸ“ Found {len(json_files)} results file(s)")
    
    # Use the most recent file
    latest_file = max(json_files, key=lambda x: os.path.getctime(os.path.join(results_dir, x)))
    json_path = os.path.join(results_dir, latest_file)
    
    print(f"ğŸ“Š Processing: {latest_file}")
    print()
    
    # Generate professional report
    print("ğŸ¨ Generating Professional Report...")
    try:
        report_path = generate_professional_report_from_json(json_path)
        
        if report_path:
            print(f"âœ… Professional report generated successfully!")
            print(f"ğŸ“„ Report location: {report_path}")
            print()
            
            # Show report features
            print("ğŸŒŸ Professional Report Features:")
            print("  ğŸ“‹ Executive Summary with performance insights")
            print("  ğŸ”§ Detailed model configuration")
            print("  ğŸ“Š Visual performance indicators and progress bars")
            print("  ğŸ’¡ Actionable recommendations")
            print("  ğŸ“– Enhanced sample analysis")
            print("  ğŸ¯ Color-coded performance badges")
            print()
            
            # Show how to view the report
            print("ğŸ‘€ To view the report:")
            print(f"  cat '{report_path}'")
            print("  # or open in your favorite markdown viewer")
            print()
            
            # Show comparison with standard reports
            print("ğŸ”„ Report Format Comparison:")
            print("  Professional Format (Default):")
            print("    âœ¨ Modern visual design with emojis")
            print("    ğŸ“ˆ Performance charts and progress bars")
            print("    ğŸ¯ Executive summary and insights")
            print("    ğŸ’¡ Actionable recommendations")
            print()
            print("  Standard Format:")
            print("    ğŸ“ Traditional markdown tables")
            print("    ğŸ”¢ Basic metrics display")
            print("    âš¡ Lightweight and fast")
            print()
            
            # Show CLI usage examples
            print("ğŸš€ CLI Usage Examples:")
            print()
            print("  # Professional format (default)")
            print("  python llm_eval_cli.py \\")
            print("    --model hf \\")
            print("    --model_name mistralai/Mistral-7B-v0.1 \\")
            print("    --tasks arc_easy \\")
            print("    --report_format professional")
            print()
            print("  # Standard format")
            print("  python llm_eval_cli.py \\")
            print("    --model hf \\")
            print("    --model_name mistralai/Mistral-7B-v0.1 \\")
            print("    --tasks arc_easy \\")
            print("    --report_format standard")
            print()
            
        else:
            print("âŒ Failed to generate professional report")
            
    except Exception as e:
        print(f"âŒ Error generating professional report: {e}")

def show_report_features():
    """Show the key features of professional reports."""
    
    print("ğŸ¨ Professional Report Features Overview")
    print("=" * 50)
    print()
    
    features = [
        ("ğŸ“‹ Executive Summary", "Overall performance assessment with key insights and recommendations"),
        ("ğŸ”§ Model Configuration", "Comprehensive technical specifications including architecture and setup"),
        ("ğŸ“Š Performance Overview", "Visual performance indicators with color-coded badges and progress bars"),
        ("ğŸ’¡ Smart Recommendations", "Actionable insights based on performance analysis"),
        ("ğŸ“– Enhanced Sample Analysis", "Beautifully formatted questions, choices, and model responses"),
        ("ğŸ¯ Performance Badges", "ğŸŸ¢ Excellent (70%+), ğŸŸ¡ Good (50-70%), ğŸ”´ Needs Improvement (<50%)"),
        ("ğŸ“ˆ Visual Charts", "ASCII-based performance comparison charts"),
        ("ğŸ·ï¸ Professional Formatting", "Modern design with emojis, sections, and clear hierarchy")
    ]
    
    for feature, description in features:
        print(f"{feature}")
        print(f"  {description}")
        print()

if __name__ == "__main__":
    if len(sys.argv) > 1 and sys.argv[1] == "--features":
        show_report_features()
    else:
        demo_professional_reports() 