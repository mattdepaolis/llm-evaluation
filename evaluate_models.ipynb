{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "import os\n",
    "\n",
    "# Use environment variable for Hugging Face token\n",
    "# Set HUGGINGFACE_HUB_TOKEN environment variable before running\n",
    "token = os.getenv('HUGGINGFACE_HUB_TOKEN')\n",
    "if token:\n",
    "    login(token=token)\n",
    "else:\n",
    "    print(\"Warning: HUGGINGFACE_HUB_TOKEN environment variable not set. Some models may not be accessible.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Variable Verification:\n",
      "  Model Name: Qwen/Qwen3-8B\n",
      "  Model Type: hf\n",
      "  Tasks: ['leaderboard']\n",
      "  Quantize: True\n",
      "==================================================\n",
      "Expanding task group 'LEADERBOARD' to 39 individual tasks\n",
      "Auto-generated output path: /workspace/llm-evaluation/evaluations/Qwen3-8B_2025-10-09_150315/results.json\n",
      "Using enhanced evaluation to capture model text outputs...\n",
      "Enhanced evaluation - capturing model text outputs\n",
      "Evaluating model type: hf\n",
      "Model: Qwen/Qwen3-8B\n",
      "Tasks: leaderboard\n",
      "Device: cuda, Few-shot examples: 0\n",
      "Batch size: 1\n",
      "Using 1 samples per task\n",
      "Using quantization method: 4bit\n",
      "Using default few-shot settings for each task:\n",
      "  - BBH tasks: 3-shot\n",
      "  - GPQA tasks: 0-shot\n",
      "  - MMLU-Pro tasks: 5-shot\n",
      "  - MUSR tasks: 0-shot\n",
      "  - IFEval tasks: 0-shot\n",
      "  - Math-lvl-5 tasks: 4-shot\n",
      "Setting random seed to 42 for reproducible sample selection\n",
      "Starting enhanced evaluation on 1 tasks: leaderboard\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n",
      "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "94ebf3c8835b4591a2326b58c30be987",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1079628f2ff14010bfa858a63ed9fff4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "precalculus/train-00000-of-00001.parquet:   0%|          | 0.00/354k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cc7e40a376bd4dbaadfbe6bd12e6b55d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "precalculus/test-00000-of-00001.parquet:   0%|          | 0.00/242k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0fb58778166f4f16aac4fcaa1c8fcdbd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/746 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9c44f083901d44eda66f2fb41db84e6b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/546 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "84723d9e12944efcbf6c15f7120be8d8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/546 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ea107eb2306041e88f1460fb5c3df117",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/135 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f62be8f9be5649e4a52e3b169c953119",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "prealgebra/train-00000-of-00001.parquet:   0%|          | 0.00/384k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "661408deffb446d8ad421df2d12d9a25",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "prealgebra/test-00000-of-00001.parquet:   0%|          | 0.00/268k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "193226f5de6442288a490dae86124077",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/1205 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6a26155c72ed4441841da5252bcfd68d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/871 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5275c53107bb4b19bef5d3d88585d505",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/871 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "653b684ebc854eddb7f0ab8d5913d6a1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/193 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a2c52461623543639e1e61b907346fbe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "number_theory/train-00000-of-00001.parqu(‚Ä¶):   0%|          | 0.00/309k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "94fc8a83f0114989863ead8e62ef4d4b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "number_theory/test-00000-of-00001.parque(‚Ä¶):   0%|          | 0.00/182k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "022b28a8680e4d3d88e8ff45e6a366f7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/869 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ddf607f7733f4873a982b6b7da5d77a7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/540 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6b34aaca4ffd43a98d1de96a91bc78af",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/540 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cbf5f14b625e434fa537f16c497114a2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/154 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "33134ae491ec471f885e0bbe42590d8a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "intermediate_algebra/train-00000-of-0000(‚Ä¶):   0%|          | 0.00/575k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "feba4e6b899441fb97d691a5037ab84c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "intermediate_algebra/test-00000-of-00001(‚Ä¶):   0%|          | 0.00/395k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f0e87aed6ec04f9c80030a2fff5a3858",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/1295 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "405c21d4252a44a38c16be5563ccea3c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/903 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fed5ca08c64a480cb54dbdef9d1c865c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/903 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5bd9d90756664f4091c36f1fe02d6850",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/280 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0fdd882728f64617a56979b4c47e4c87",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "geometry/train-00000-of-00001.parquet:   0%|          | 0.00/549k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cee68db5a81d47a2a571e93ec9c84fdf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "geometry/test-00000-of-00001.parquet:   0%|          | 0.00/264k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "38c596de91034056a9d90e45cba253b5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/870 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8b735b33dfd0495fa65718ea8effbbdf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/479 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f2c8dc30553944aaa3e6256b51206086",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/479 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6214b3b36abc481e88ec56f1b15e05ff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/132 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9cf2fc242c3b413781d9dc04c1e2283a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "counting_and_probability/train-00000-of-(‚Ä¶):   0%|          | 0.00/329k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "24107b6c25a64407a192d89dbdbae124",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "counting_and_probability/test-00000-of-0(‚Ä¶):   0%|          | 0.00/175k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8daa4c391bef4371beb85069ff8ddd7a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/771 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5377a09a6a904d44abfec4ce8312ec4e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/474 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "30d652288abb49ac99390161ac60a689",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/474 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ea2d5be6631b4fb5a50a88268ab7c1be",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/123 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b91f9d7f7c014f508251bb393a403f55",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "algebra/train-00000-of-00001.parquet:   0%|          | 0.00/505k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1725bd42155a48ed9b30a03a6a4b2e02",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "algebra/test-00000-of-00001.parquet:   0%|          | 0.00/353k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cf2aa6f8835842b48870ee4f3462b34b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/1744 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9775fefe6a1249e48cdc02ff14fc3c6f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/1187 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a9663bfaefad41019dbd104757e49171",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/1187 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2cc9fa5352cd42c596b74e4e77ce282f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/307 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "264cecc4142443038e4c85120fd9ce11",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "gpqa_main.csv:   0%|          | 0.00/3.21M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "97cb473ad92d459999a3956412051fa5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/448 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d49b6eca0f904ca5a826d00c97131e5c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/448 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eed4f9f829cb4f88899c5de2980ea38f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "gpqa_extended.csv:   0%|          | 0.00/4.09M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8ba7763e488f4a46b5fd725aa01fa5b8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/546 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "43cc305db60f459d92306d1e4ab47077",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/546 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "96bcc45acf0145549374e8113f009d52",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "gpqa_diamond.csv:   0%|          | 0.00/1.37M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0b3cfacbe19f4dba8a2f55d11cbdaaae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/198 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "182e6f6122004e6ea299ae304bfc92ea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/198 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "62c340047f414d708597d3357a58e81d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b5531646a9f241578e58cd58b743a77b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "data/test-00000-of-00001.parquet:   0%|          | 0.00/4.15M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4186a2d69cc24ffd9954c112eb6cdbc6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "data/validation-00000-of-00001.parquet:   0%|          | 0.00/45.3k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "042e54dcb134451fa9fe1707f5f7abda",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/12032 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d92e17b786dd45149912752938515045",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split:   0%|          | 0/70 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 9098.27it/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 502.79it/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 586.86it/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 612.31it/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 393.09it/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 405.56it/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 646.97it/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 465.62it/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 530.12it/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 593.51it/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 606.11it/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 596.54it/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 617.35it/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 484.83it/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 625.64it/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 622.67it/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 414.87it/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 560.74it/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 545.07it/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 620.18it/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 613.56it/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 583.27it/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 556.35it/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 477.93it/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 635.79it/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 1391.15it/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 1402.31it/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 1586.35it/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 622.76it/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 632.24it/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 582.70it/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 632.91it/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 656.59it/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 596.71it/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 656.28it/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 16070.13it/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 2739.58it/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 3045.97it/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 2011.66it/s]\n",
      "Running loglikelihood requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 159/159 [00:14<00:00, 10.82it/s]\n",
      "Running generate_until requests:   0%|          | 0/8 [00:00<?, ?it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 8/8 [03:00<00:00, 22.57s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing results and extracting model text outputs...\n",
      "Found samples section, processing text outputs...\n",
      "Processing text outputs for task: leaderboard_mmlu_pro (1 samples)\n",
      "Processing text outputs for task: leaderboard_bbh_boolean_expressions (1 samples)\n",
      "Processing text outputs for task: leaderboard_bbh_causal_judgement (1 samples)\n",
      "Processing text outputs for task: leaderboard_bbh_date_understanding (1 samples)\n",
      "Processing text outputs for task: leaderboard_bbh_disambiguation_qa (1 samples)\n",
      "Processing text outputs for task: leaderboard_bbh_formal_fallacies (1 samples)\n",
      "Processing text outputs for task: leaderboard_bbh_geometric_shapes (1 samples)\n",
      "Processing text outputs for task: leaderboard_bbh_hyperbaton (1 samples)\n",
      "Processing text outputs for task: leaderboard_bbh_logical_deduction_five_objects (1 samples)\n",
      "Processing text outputs for task: leaderboard_bbh_logical_deduction_seven_objects (1 samples)\n",
      "Processing text outputs for task: leaderboard_bbh_logical_deduction_three_objects (1 samples)\n",
      "Processing text outputs for task: leaderboard_bbh_movie_recommendation (1 samples)\n",
      "Processing text outputs for task: leaderboard_bbh_navigate (1 samples)\n",
      "Processing text outputs for task: leaderboard_bbh_object_counting (1 samples)\n",
      "Processing text outputs for task: leaderboard_bbh_penguins_in_a_table (1 samples)\n",
      "Processing text outputs for task: leaderboard_bbh_reasoning_about_colored_objects (1 samples)\n",
      "Processing text outputs for task: leaderboard_bbh_ruin_names (1 samples)\n",
      "Processing text outputs for task: leaderboard_bbh_salient_translation_error_detection (1 samples)\n",
      "Processing text outputs for task: leaderboard_bbh_snarks (1 samples)\n",
      "Processing text outputs for task: leaderboard_bbh_sports_understanding (1 samples)\n",
      "Processing text outputs for task: leaderboard_bbh_temporal_sequences (1 samples)\n",
      "Processing text outputs for task: leaderboard_bbh_tracking_shuffled_objects_five_objects (1 samples)\n",
      "Processing text outputs for task: leaderboard_bbh_tracking_shuffled_objects_seven_objects (1 samples)\n",
      "Processing text outputs for task: leaderboard_bbh_tracking_shuffled_objects_three_objects (1 samples)\n",
      "Processing text outputs for task: leaderboard_bbh_web_of_lies (1 samples)\n",
      "Processing text outputs for task: leaderboard_gpqa_diamond (1 samples)\n",
      "Processing text outputs for task: leaderboard_gpqa_extended (1 samples)\n",
      "Processing text outputs for task: leaderboard_gpqa_main (1 samples)\n",
      "Processing text outputs for task: leaderboard_math_algebra_hard (1 samples)\n",
      "Warning: Could not decode response for leaderboard_math_algebra_hard: 'dict' object has no attribute 'strip'\n",
      "Processing text outputs for task: leaderboard_math_counting_and_prob_hard (1 samples)\n",
      "Warning: Could not decode response for leaderboard_math_counting_and_prob_hard: 'dict' object has no attribute 'strip'\n",
      "Processing text outputs for task: leaderboard_math_geometry_hard (1 samples)\n",
      "Warning: Could not decode response for leaderboard_math_geometry_hard: 'dict' object has no attribute 'strip'\n",
      "Processing text outputs for task: leaderboard_math_intermediate_algebra_hard (1 samples)\n",
      "Warning: Could not decode response for leaderboard_math_intermediate_algebra_hard: 'dict' object has no attribute 'strip'\n",
      "Processing text outputs for task: leaderboard_math_num_theory_hard (1 samples)\n",
      "Warning: Could not decode response for leaderboard_math_num_theory_hard: 'dict' object has no attribute 'strip'\n",
      "Processing text outputs for task: leaderboard_math_prealgebra_hard (1 samples)\n",
      "Warning: Could not decode response for leaderboard_math_prealgebra_hard: 'dict' object has no attribute 'strip'\n",
      "Processing text outputs for task: leaderboard_math_precalculus_hard (1 samples)\n",
      "Warning: Could not decode response for leaderboard_math_precalculus_hard: 'dict' object has no attribute 'strip'\n",
      "Processing text outputs for task: leaderboard_ifeval (1 samples)\n",
      "Warning: Could not decode response for leaderboard_ifeval: 'dict' object has no attribute 'strip'\n",
      "Processing text outputs for task: leaderboard_musr_murder_mysteries (1 samples)\n",
      "Processing text outputs for task: leaderboard_musr_object_placements (1 samples)\n",
      "Processing text outputs for task: leaderboard_musr_team_allocation (1 samples)\n",
      "Results saved to /workspace/llm-evaluation/evaluations/Qwen3-8B_2025-10-09_150315/results.json\n",
      "‚úÖ Professional report generated: /workspace/llm-evaluation/evaluations/Qwen3-8B_2025-10-09_150315/report.md\n",
      "‚úÖ Task report generated: /workspace/llm-evaluation/evaluations/Qwen3-8B_2025-10-09_150315/task_reports/leaderboard_mmlu_pro_report.md\n",
      "‚úÖ Task report generated: /workspace/llm-evaluation/evaluations/Qwen3-8B_2025-10-09_150315/task_reports/leaderboard_bbh_boolean_expressions_report.md\n",
      "‚úÖ Task report generated: /workspace/llm-evaluation/evaluations/Qwen3-8B_2025-10-09_150315/task_reports/leaderboard_bbh_causal_judgement_report.md\n",
      "‚úÖ Task report generated: /workspace/llm-evaluation/evaluations/Qwen3-8B_2025-10-09_150315/task_reports/leaderboard_bbh_date_understanding_report.md\n",
      "‚úÖ Task report generated: /workspace/llm-evaluation/evaluations/Qwen3-8B_2025-10-09_150315/task_reports/leaderboard_bbh_disambiguation_qa_report.md\n",
      "‚úÖ Task report generated: /workspace/llm-evaluation/evaluations/Qwen3-8B_2025-10-09_150315/task_reports/leaderboard_bbh_formal_fallacies_report.md\n",
      "‚úÖ Task report generated: /workspace/llm-evaluation/evaluations/Qwen3-8B_2025-10-09_150315/task_reports/leaderboard_bbh_geometric_shapes_report.md\n",
      "‚úÖ Task report generated: /workspace/llm-evaluation/evaluations/Qwen3-8B_2025-10-09_150315/task_reports/leaderboard_bbh_hyperbaton_report.md\n",
      "‚úÖ Task report generated: /workspace/llm-evaluation/evaluations/Qwen3-8B_2025-10-09_150315/task_reports/leaderboard_bbh_logical_deduction_five_objects_report.md\n",
      "‚úÖ Task report generated: /workspace/llm-evaluation/evaluations/Qwen3-8B_2025-10-09_150315/task_reports/leaderboard_bbh_logical_deduction_seven_objects_report.md\n",
      "‚úÖ Task report generated: /workspace/llm-evaluation/evaluations/Qwen3-8B_2025-10-09_150315/task_reports/leaderboard_bbh_logical_deduction_three_objects_report.md\n",
      "‚úÖ Task report generated: /workspace/llm-evaluation/evaluations/Qwen3-8B_2025-10-09_150315/task_reports/leaderboard_bbh_movie_recommendation_report.md\n",
      "‚úÖ Task report generated: /workspace/llm-evaluation/evaluations/Qwen3-8B_2025-10-09_150315/task_reports/leaderboard_bbh_navigate_report.md\n",
      "‚úÖ Task report generated: /workspace/llm-evaluation/evaluations/Qwen3-8B_2025-10-09_150315/task_reports/leaderboard_bbh_object_counting_report.md\n",
      "‚úÖ Task report generated: /workspace/llm-evaluation/evaluations/Qwen3-8B_2025-10-09_150315/task_reports/leaderboard_bbh_penguins_in_a_table_report.md\n",
      "‚úÖ Task report generated: /workspace/llm-evaluation/evaluations/Qwen3-8B_2025-10-09_150315/task_reports/leaderboard_bbh_reasoning_about_colored_objects_report.md\n",
      "‚úÖ Task report generated: /workspace/llm-evaluation/evaluations/Qwen3-8B_2025-10-09_150315/task_reports/leaderboard_bbh_ruin_names_report.md\n",
      "‚úÖ Task report generated: /workspace/llm-evaluation/evaluations/Qwen3-8B_2025-10-09_150315/task_reports/leaderboard_bbh_salient_translation_error_detection_report.md\n",
      "‚úÖ Task report generated: /workspace/llm-evaluation/evaluations/Qwen3-8B_2025-10-09_150315/task_reports/leaderboard_bbh_snarks_report.md\n",
      "‚úÖ Task report generated: /workspace/llm-evaluation/evaluations/Qwen3-8B_2025-10-09_150315/task_reports/leaderboard_bbh_sports_understanding_report.md\n",
      "‚úÖ Task report generated: /workspace/llm-evaluation/evaluations/Qwen3-8B_2025-10-09_150315/task_reports/leaderboard_bbh_temporal_sequences_report.md\n",
      "‚úÖ Task report generated: /workspace/llm-evaluation/evaluations/Qwen3-8B_2025-10-09_150315/task_reports/leaderboard_bbh_tracking_shuffled_objects_five_objects_report.md\n",
      "‚úÖ Task report generated: /workspace/llm-evaluation/evaluations/Qwen3-8B_2025-10-09_150315/task_reports/leaderboard_bbh_tracking_shuffled_objects_seven_objects_report.md\n",
      "‚úÖ Task report generated: /workspace/llm-evaluation/evaluations/Qwen3-8B_2025-10-09_150315/task_reports/leaderboard_bbh_tracking_shuffled_objects_three_objects_report.md\n",
      "‚úÖ Task report generated: /workspace/llm-evaluation/evaluations/Qwen3-8B_2025-10-09_150315/task_reports/leaderboard_bbh_web_of_lies_report.md\n",
      "‚úÖ Task report generated: /workspace/llm-evaluation/evaluations/Qwen3-8B_2025-10-09_150315/task_reports/leaderboard_gpqa_diamond_report.md\n",
      "‚úÖ Task report generated: /workspace/llm-evaluation/evaluations/Qwen3-8B_2025-10-09_150315/task_reports/leaderboard_gpqa_extended_report.md\n",
      "‚úÖ Task report generated: /workspace/llm-evaluation/evaluations/Qwen3-8B_2025-10-09_150315/task_reports/leaderboard_gpqa_main_report.md\n",
      "‚úÖ Task report generated: /workspace/llm-evaluation/evaluations/Qwen3-8B_2025-10-09_150315/task_reports/leaderboard_math_algebra_hard_report.md\n",
      "‚úÖ Task report generated: /workspace/llm-evaluation/evaluations/Qwen3-8B_2025-10-09_150315/task_reports/leaderboard_math_counting_and_prob_hard_report.md\n",
      "‚úÖ Task report generated: /workspace/llm-evaluation/evaluations/Qwen3-8B_2025-10-09_150315/task_reports/leaderboard_math_geometry_hard_report.md\n",
      "‚úÖ Task report generated: /workspace/llm-evaluation/evaluations/Qwen3-8B_2025-10-09_150315/task_reports/leaderboard_math_intermediate_algebra_hard_report.md\n",
      "‚úÖ Task report generated: /workspace/llm-evaluation/evaluations/Qwen3-8B_2025-10-09_150315/task_reports/leaderboard_math_num_theory_hard_report.md\n",
      "‚úÖ Task report generated: /workspace/llm-evaluation/evaluations/Qwen3-8B_2025-10-09_150315/task_reports/leaderboard_math_prealgebra_hard_report.md\n",
      "‚úÖ Task report generated: /workspace/llm-evaluation/evaluations/Qwen3-8B_2025-10-09_150315/task_reports/leaderboard_math_precalculus_hard_report.md\n",
      "‚úÖ Task report generated: /workspace/llm-evaluation/evaluations/Qwen3-8B_2025-10-09_150315/task_reports/leaderboard_ifeval_report.md\n",
      "‚úÖ Task report generated: /workspace/llm-evaluation/evaluations/Qwen3-8B_2025-10-09_150315/task_reports/leaderboard_musr_murder_mysteries_report.md\n",
      "‚úÖ Task report generated: /workspace/llm-evaluation/evaluations/Qwen3-8B_2025-10-09_150315/task_reports/leaderboard_musr_object_placements_report.md\n",
      "‚úÖ Task report generated: /workspace/llm-evaluation/evaluations/Qwen3-8B_2025-10-09_150315/task_reports/leaderboard_musr_team_allocation_report.md\n",
      "‚úÖ Generated 39 task-specific reports\n",
      "‚úÖ Professional report generated: /workspace/llm-evaluation/evaluations/Qwen3-8B_2025-10-09_150315/report.md\n",
      "‚úÖ Organized evaluation saved to: /workspace/llm-evaluation/evaluations/Qwen3-8B_2025-10-09_150315\n",
      "Clearing GPU memory...\n",
      "GPU memory cleared.\n",
      "Results saved to: /workspace/llm-evaluation/evaluations/Qwen3-8B_2025-10-09_150315/results.json\n",
      "Report was not generated. Check if there were any errors during evaluation.\n"
     ]
    }
   ],
   "source": [
    "# ‚ö†Ô∏è IMPORTANT: Clear any cached variables to ensure fresh execution\n",
    "%reset -f\n",
    "\n",
    "from llm_eval import evaluate_model\n",
    "from llm_eval.main import generate_output_path, process_tasks\n",
    "import os\n",
    "from llm_eval.reporting.report_generator import get_reports_dir\n",
    "\n",
    "# Auto-generate output path like the CLI does\n",
    "tasks = [\"leaderboard\"]\n",
    "model_name = \"Qwen/Qwen3-8B\"\n",
    "model_type = \"hf\" #vllm or hf\n",
    "quantize = True # False for no quantization\n",
    "quantization_method = \"4bit\"\n",
    "\n",
    "# üîç Verify the variables are set correctly (DEBUGGING)\n",
    "print(f\"üîç Variable Verification:\")\n",
    "print(f\"  Model Name: {model_name}\")\n",
    "print(f\"  Model Type: {model_type}\")\n",
    "print(f\"  Tasks: {tasks}\")\n",
    "print(f\"  Quantize: {quantize}\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Process tasks to get the actual task list\n",
    "processed_tasks = process_tasks(tasks)\n",
    "\n",
    "# Generate output path\n",
    "output_path = generate_output_path(\n",
    "    model_name=model_name,\n",
    "    model_type=model_type,\n",
    "    tasks=processed_tasks,\n",
    "    quantize=quantize,\n",
    "    quantization_method=quantization_method\n",
    ")\n",
    "\n",
    "print(f\"Auto-generated output path: {output_path}\")\n",
    "\n",
    "# Run the enhanced evaluation (now captures model text outputs by default)\n",
    "results, output_path = evaluate_model(\n",
    "    model_type=model_type,\n",
    "    model_name=model_name,\n",
    "    tasks=tasks,\n",
    "    num_samples=1, # 1 for no few-shot, 10 for few-shot\n",
    "    device=\"cuda\",\n",
    "    quantize=quantize,\n",
    "    quantization_method=quantization_method,\n",
    "    output_path=output_path,  # Now we provide the auto-generated path\n",
    "    preserve_default_fewshot=True,  # This ensures the correct few-shot settings for each benchmark task\n",
    "    capture_text_outputs=True,  # NEW: Capture model text outputs for comparison with targets\n",
    "    seed=42  # NEW: Ensure reproducible sample selection\n",
    ")\n",
    "\n",
    "# Print the paths to the results and report\n",
    "print(f\"Results saved to: {output_path}\")\n",
    "\n",
    "# The report path is derived from the output path\n",
    "if output_path:\n",
    "    # Get the base filename without extension\n",
    "    basename = os.path.basename(output_path)\n",
    "    basename = os.path.splitext(basename)[0]\n",
    "\n",
    "    # Construct the report path\n",
    "    reports_dir = get_reports_dir()\n",
    "    report_path = os.path.join(reports_dir, f\"{basename}_report.md\")\n",
    "\n",
    "    if os.path.exists(report_path):\n",
    "        print(f\"Report generated at: {report_path}\")\n",
    "    else:\n",
    "        print(\"Report was not generated. Check if there were any errors during evaluation.\")\n",
    "else:\n",
    "    print(\"No output path available. Cannot check for report.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expanding task group 'LEADERBOARD' to 39 individual tasks\n",
      "Running leaderboard tasks with their default few-shot settings\n",
      "Auto-generated output path: /workspace/llm-evaluation/evaluations/Ministral-8B-Instruct-2410_2025-10-09_151651/results.json\n",
      "Using enhanced evaluation to capture model text outputs...\n",
      "Enhanced evaluation - capturing model text outputs\n",
      "Evaluating model type: hf\n",
      "Model: mistralai/Ministral-8B-Instruct-2410\n",
      "Tasks: leaderboard_bbh_boolean_expressions, leaderboard_bbh_causal_judgement, leaderboard_bbh_date_understanding, leaderboard_bbh_disambiguation_qa, leaderboard_bbh_formal_fallacies, leaderboard_bbh_geometric_shapes, leaderboard_bbh_hyperbaton, leaderboard_bbh_logical_deduction_five_objects, leaderboard_bbh_logical_deduction_seven_objects, leaderboard_bbh_logical_deduction_three_objects, leaderboard_bbh_movie_recommendation, leaderboard_bbh_navigate, leaderboard_bbh_object_counting, leaderboard_bbh_penguins_in_a_table, leaderboard_bbh_reasoning_about_colored_objects, leaderboard_bbh_ruin_names, leaderboard_bbh_salient_translation_error_detection, leaderboard_bbh_snarks, leaderboard_bbh_sports_understanding, leaderboard_bbh_temporal_sequences, leaderboard_bbh_tracking_shuffled_objects_five_objects, leaderboard_bbh_tracking_shuffled_objects_seven_objects, leaderboard_bbh_tracking_shuffled_objects_three_objects, leaderboard_bbh_web_of_lies, leaderboard_gpqa_main, leaderboard_gpqa_diamond, leaderboard_gpqa_extended, leaderboard_mmlu_pro, leaderboard_musr_murder_mysteries, leaderboard_musr_object_placements, leaderboard_musr_team_allocation, leaderboard_ifeval, leaderboard_math_algebra_hard, leaderboard_math_counting_and_prob_hard, leaderboard_math_geometry_hard, leaderboard_math_intermediate_algebra_hard, leaderboard_math_num_theory_hard, leaderboard_math_prealgebra_hard, leaderboard_math_precalculus_hard\n",
      "Device: cuda, Few-shot examples: 0\n",
      "Batch size: 1\n",
      "Using 10 samples per task\n",
      "Using quantization method: 4bit\n",
      "Using default few-shot settings for each task:\n",
      "  - BBH tasks: 3-shot\n",
      "  - GPQA tasks: 0-shot\n",
      "  - MMLU-Pro tasks: 5-shot\n",
      "  - MUSR tasks: 0-shot\n",
      "  - IFEval tasks: 0-shot\n",
      "  - Math-lvl-5 tasks: 4-shot\n",
      "Setting random seed to 42 for reproducible sample selection\n",
      "Starting enhanced evaluation on 39 tasks: leaderboard_bbh_boolean_expressions, leaderboard_bbh_causal_judgement, leaderboard_bbh_date_understanding, leaderboard_bbh_disambiguation_qa, leaderboard_bbh_formal_fallacies, leaderboard_bbh_geometric_shapes, leaderboard_bbh_hyperbaton, leaderboard_bbh_logical_deduction_five_objects, leaderboard_bbh_logical_deduction_seven_objects, leaderboard_bbh_logical_deduction_three_objects, leaderboard_bbh_movie_recommendation, leaderboard_bbh_navigate, leaderboard_bbh_object_counting, leaderboard_bbh_penguins_in_a_table, leaderboard_bbh_reasoning_about_colored_objects, leaderboard_bbh_ruin_names, leaderboard_bbh_salient_translation_error_detection, leaderboard_bbh_snarks, leaderboard_bbh_sports_understanding, leaderboard_bbh_temporal_sequences, leaderboard_bbh_tracking_shuffled_objects_five_objects, leaderboard_bbh_tracking_shuffled_objects_seven_objects, leaderboard_bbh_tracking_shuffled_objects_three_objects, leaderboard_bbh_web_of_lies, leaderboard_gpqa_main, leaderboard_gpqa_diamond, leaderboard_gpqa_extended, leaderboard_mmlu_pro, leaderboard_musr_murder_mysteries, leaderboard_musr_object_placements, leaderboard_musr_team_allocation, leaderboard_ifeval, leaderboard_math_algebra_hard, leaderboard_math_counting_and_prob_hard, leaderboard_math_geometry_hard, leaderboard_math_intermediate_algebra_hard, leaderboard_math_num_theory_hard, leaderboard_math_prealgebra_hard, leaderboard_math_precalculus_hard\n",
      "pretrained=pretrained=mistralai/Ministral-8B-Instruct-2410,load_in_4bit=True appears to be an instruct or chat variant but chat template is\n",
      "        not applied. Recommend setting `apply_chat_template` (optionally `fewshot_as_multiturn`).\n",
      "You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama_fast.LlamaTokenizerFast'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message.\n",
      "`torch_dtype` is deprecated! Use `dtype` instead!\n",
      "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n",
      "Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4/4 [00:27<00:00,  6.94s/it]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [00:00<00:00, 726.65it/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [00:00<00:00, 698.53it/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [00:00<00:00, 758.56it/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [00:00<00:00, 732.39it/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [00:00<00:00, 698.75it/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [00:00<00:00, 726.51it/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [00:00<00:00, 751.28it/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [00:00<00:00, 140277.73it/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [00:00<00:00, 4099.60it/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [00:00<00:00, 4662.93it/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [00:00<00:00, 4771.13it/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [00:00<00:00, 35484.81it/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [00:00<00:00, 2175.92it/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [00:00<00:00, 2318.83it/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [00:00<00:00, 2528.21it/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [00:00<00:00, 722.65it/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [00:00<00:00, 741.55it/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [00:00<00:00, 670.84it/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [00:00<00:00, 673.44it/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [00:00<00:00, 727.80it/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [00:00<00:00, 747.61it/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [00:00<00:00, 730.26it/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [00:00<00:00, 705.01it/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [00:00<00:00, 707.68it/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [00:00<00:00, 675.20it/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [00:00<00:00, 663.83it/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [00:00<00:00, 661.19it/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [00:00<00:00, 732.78it/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [00:00<00:00, 707.20it/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [00:00<00:00, 711.01it/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [00:00<00:00, 698.08it/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [00:00<00:00, 711.70it/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [00:00<00:00, 735.42it/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [00:00<00:00, 712.41it/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [00:00<00:00, 743.39it/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [00:00<00:00, 706.39it/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [00:00<00:00, 699.87it/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [00:00<00:00, 657.52it/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [00:00<00:00, 743.79it/s]\n",
      "Running generate_until requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 80/80 [13:04<00:00,  9.81s/it]\n",
      "Running loglikelihood requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1603/1603 [02:05<00:00, 12.82it/s]\n",
      "Processing results and extracting model text outputs...\n",
      "Found samples section, processing text outputs...\n",
      "Processing text outputs for task: leaderboard_math_precalculus_hard (10 samples)\n",
      "Warning: Could not decode response for leaderboard_math_precalculus_hard: 'dict' object has no attribute 'strip'\n",
      "Warning: Could not decode response for leaderboard_math_precalculus_hard: 'dict' object has no attribute 'strip'\n",
      "Warning: Could not decode response for leaderboard_math_precalculus_hard: 'dict' object has no attribute 'strip'\n",
      "Warning: Could not decode response for leaderboard_math_precalculus_hard: 'dict' object has no attribute 'strip'\n",
      "Warning: Could not decode response for leaderboard_math_precalculus_hard: 'dict' object has no attribute 'strip'\n",
      "Warning: Could not decode response for leaderboard_math_precalculus_hard: 'dict' object has no attribute 'strip'\n",
      "Warning: Could not decode response for leaderboard_math_precalculus_hard: 'dict' object has no attribute 'strip'\n",
      "Warning: Could not decode response for leaderboard_math_precalculus_hard: 'dict' object has no attribute 'strip'\n",
      "Warning: Could not decode response for leaderboard_math_precalculus_hard: 'dict' object has no attribute 'strip'\n",
      "Warning: Could not decode response for leaderboard_math_precalculus_hard: 'dict' object has no attribute 'strip'\n",
      "Processing text outputs for task: leaderboard_math_prealgebra_hard (10 samples)\n",
      "Warning: Could not decode response for leaderboard_math_prealgebra_hard: 'dict' object has no attribute 'strip'\n",
      "Warning: Could not decode response for leaderboard_math_prealgebra_hard: 'dict' object has no attribute 'strip'\n",
      "Warning: Could not decode response for leaderboard_math_prealgebra_hard: 'dict' object has no attribute 'strip'\n",
      "Warning: Could not decode response for leaderboard_math_prealgebra_hard: 'dict' object has no attribute 'strip'\n",
      "Warning: Could not decode response for leaderboard_math_prealgebra_hard: 'dict' object has no attribute 'strip'\n",
      "Warning: Could not decode response for leaderboard_math_prealgebra_hard: 'dict' object has no attribute 'strip'\n",
      "Warning: Could not decode response for leaderboard_math_prealgebra_hard: 'dict' object has no attribute 'strip'\n",
      "Warning: Could not decode response for leaderboard_math_prealgebra_hard: 'dict' object has no attribute 'strip'\n",
      "Warning: Could not decode response for leaderboard_math_prealgebra_hard: 'dict' object has no attribute 'strip'\n",
      "Warning: Could not decode response for leaderboard_math_prealgebra_hard: 'dict' object has no attribute 'strip'\n",
      "Processing text outputs for task: leaderboard_math_num_theory_hard (10 samples)\n",
      "Warning: Could not decode response for leaderboard_math_num_theory_hard: 'dict' object has no attribute 'strip'\n",
      "Warning: Could not decode response for leaderboard_math_num_theory_hard: 'dict' object has no attribute 'strip'\n",
      "Warning: Could not decode response for leaderboard_math_num_theory_hard: 'dict' object has no attribute 'strip'\n",
      "Warning: Could not decode response for leaderboard_math_num_theory_hard: 'dict' object has no attribute 'strip'\n",
      "Warning: Could not decode response for leaderboard_math_num_theory_hard: 'dict' object has no attribute 'strip'\n",
      "Warning: Could not decode response for leaderboard_math_num_theory_hard: 'dict' object has no attribute 'strip'\n",
      "Warning: Could not decode response for leaderboard_math_num_theory_hard: 'dict' object has no attribute 'strip'\n",
      "Warning: Could not decode response for leaderboard_math_num_theory_hard: 'dict' object has no attribute 'strip'\n",
      "Warning: Could not decode response for leaderboard_math_num_theory_hard: 'dict' object has no attribute 'strip'\n",
      "Warning: Could not decode response for leaderboard_math_num_theory_hard: 'dict' object has no attribute 'strip'\n",
      "Processing text outputs for task: leaderboard_math_intermediate_algebra_hard (10 samples)\n",
      "Warning: Could not decode response for leaderboard_math_intermediate_algebra_hard: 'dict' object has no attribute 'strip'\n",
      "Warning: Could not decode response for leaderboard_math_intermediate_algebra_hard: 'dict' object has no attribute 'strip'\n",
      "Warning: Could not decode response for leaderboard_math_intermediate_algebra_hard: 'dict' object has no attribute 'strip'\n",
      "Warning: Could not decode response for leaderboard_math_intermediate_algebra_hard: 'dict' object has no attribute 'strip'\n",
      "Warning: Could not decode response for leaderboard_math_intermediate_algebra_hard: 'dict' object has no attribute 'strip'\n",
      "Warning: Could not decode response for leaderboard_math_intermediate_algebra_hard: 'dict' object has no attribute 'strip'\n",
      "Warning: Could not decode response for leaderboard_math_intermediate_algebra_hard: 'dict' object has no attribute 'strip'\n",
      "Warning: Could not decode response for leaderboard_math_intermediate_algebra_hard: 'dict' object has no attribute 'strip'\n",
      "Warning: Could not decode response for leaderboard_math_intermediate_algebra_hard: 'dict' object has no attribute 'strip'\n",
      "Warning: Could not decode response for leaderboard_math_intermediate_algebra_hard: 'dict' object has no attribute 'strip'\n",
      "Processing text outputs for task: leaderboard_math_geometry_hard (10 samples)\n",
      "Warning: Could not decode response for leaderboard_math_geometry_hard: 'dict' object has no attribute 'strip'\n",
      "Warning: Could not decode response for leaderboard_math_geometry_hard: 'dict' object has no attribute 'strip'\n",
      "Warning: Could not decode response for leaderboard_math_geometry_hard: 'dict' object has no attribute 'strip'\n",
      "Warning: Could not decode response for leaderboard_math_geometry_hard: 'dict' object has no attribute 'strip'\n",
      "Warning: Could not decode response for leaderboard_math_geometry_hard: 'dict' object has no attribute 'strip'\n",
      "Warning: Could not decode response for leaderboard_math_geometry_hard: 'dict' object has no attribute 'strip'\n",
      "Warning: Could not decode response for leaderboard_math_geometry_hard: 'dict' object has no attribute 'strip'\n",
      "Warning: Could not decode response for leaderboard_math_geometry_hard: 'dict' object has no attribute 'strip'\n",
      "Warning: Could not decode response for leaderboard_math_geometry_hard: 'dict' object has no attribute 'strip'\n",
      "Warning: Could not decode response for leaderboard_math_geometry_hard: 'dict' object has no attribute 'strip'\n",
      "Processing text outputs for task: leaderboard_math_counting_and_prob_hard (10 samples)\n",
      "Warning: Could not decode response for leaderboard_math_counting_and_prob_hard: 'dict' object has no attribute 'strip'\n",
      "Warning: Could not decode response for leaderboard_math_counting_and_prob_hard: 'dict' object has no attribute 'strip'\n",
      "Warning: Could not decode response for leaderboard_math_counting_and_prob_hard: 'dict' object has no attribute 'strip'\n",
      "Warning: Could not decode response for leaderboard_math_counting_and_prob_hard: 'dict' object has no attribute 'strip'\n",
      "Warning: Could not decode response for leaderboard_math_counting_and_prob_hard: 'dict' object has no attribute 'strip'\n",
      "Warning: Could not decode response for leaderboard_math_counting_and_prob_hard: 'dict' object has no attribute 'strip'\n",
      "Warning: Could not decode response for leaderboard_math_counting_and_prob_hard: 'dict' object has no attribute 'strip'\n",
      "Warning: Could not decode response for leaderboard_math_counting_and_prob_hard: 'dict' object has no attribute 'strip'\n",
      "Warning: Could not decode response for leaderboard_math_counting_and_prob_hard: 'dict' object has no attribute 'strip'\n",
      "Warning: Could not decode response for leaderboard_math_counting_and_prob_hard: 'dict' object has no attribute 'strip'\n",
      "Processing text outputs for task: leaderboard_math_algebra_hard (10 samples)\n",
      "Warning: Could not decode response for leaderboard_math_algebra_hard: 'dict' object has no attribute 'strip'\n",
      "Warning: Could not decode response for leaderboard_math_algebra_hard: 'dict' object has no attribute 'strip'\n",
      "Warning: Could not decode response for leaderboard_math_algebra_hard: 'dict' object has no attribute 'strip'\n",
      "Warning: Could not decode response for leaderboard_math_algebra_hard: 'dict' object has no attribute 'strip'\n",
      "Warning: Could not decode response for leaderboard_math_algebra_hard: 'dict' object has no attribute 'strip'\n",
      "Warning: Could not decode response for leaderboard_math_algebra_hard: 'dict' object has no attribute 'strip'\n",
      "Warning: Could not decode response for leaderboard_math_algebra_hard: 'dict' object has no attribute 'strip'\n",
      "Warning: Could not decode response for leaderboard_math_algebra_hard: 'dict' object has no attribute 'strip'\n",
      "Warning: Could not decode response for leaderboard_math_algebra_hard: 'dict' object has no attribute 'strip'\n",
      "Warning: Could not decode response for leaderboard_math_algebra_hard: 'dict' object has no attribute 'strip'\n",
      "Processing text outputs for task: leaderboard_ifeval (10 samples)\n",
      "Warning: Could not decode response for leaderboard_ifeval: 'dict' object has no attribute 'strip'\n",
      "Warning: Could not decode response for leaderboard_ifeval: 'dict' object has no attribute 'strip'\n",
      "Warning: Could not decode response for leaderboard_ifeval: 'dict' object has no attribute 'strip'\n",
      "Warning: Could not decode response for leaderboard_ifeval: 'dict' object has no attribute 'strip'\n",
      "Warning: Could not decode response for leaderboard_ifeval: 'dict' object has no attribute 'strip'\n",
      "Warning: Could not decode response for leaderboard_ifeval: 'dict' object has no attribute 'strip'\n",
      "Warning: Could not decode response for leaderboard_ifeval: 'dict' object has no attribute 'strip'\n",
      "Warning: Could not decode response for leaderboard_ifeval: 'dict' object has no attribute 'strip'\n",
      "Warning: Could not decode response for leaderboard_ifeval: 'dict' object has no attribute 'strip'\n",
      "Warning: Could not decode response for leaderboard_ifeval: 'dict' object has no attribute 'strip'\n",
      "Processing text outputs for task: leaderboard_musr_team_allocation (10 samples)\n",
      "Processing text outputs for task: leaderboard_musr_object_placements (10 samples)\n",
      "Processing text outputs for task: leaderboard_musr_murder_mysteries (10 samples)\n",
      "Processing text outputs for task: leaderboard_mmlu_pro (10 samples)\n",
      "Processing text outputs for task: leaderboard_gpqa_extended (10 samples)\n",
      "Processing text outputs for task: leaderboard_gpqa_diamond (10 samples)\n",
      "Processing text outputs for task: leaderboard_gpqa_main (10 samples)\n",
      "Processing text outputs for task: leaderboard_bbh_web_of_lies (10 samples)\n",
      "Processing text outputs for task: leaderboard_bbh_tracking_shuffled_objects_three_objects (10 samples)\n",
      "Processing text outputs for task: leaderboard_bbh_tracking_shuffled_objects_seven_objects (10 samples)\n",
      "Processing text outputs for task: leaderboard_bbh_tracking_shuffled_objects_five_objects (10 samples)\n",
      "Processing text outputs for task: leaderboard_bbh_temporal_sequences (10 samples)\n",
      "Processing text outputs for task: leaderboard_bbh_sports_understanding (10 samples)\n",
      "Processing text outputs for task: leaderboard_bbh_snarks (10 samples)\n",
      "Processing text outputs for task: leaderboard_bbh_salient_translation_error_detection (10 samples)\n",
      "Processing text outputs for task: leaderboard_bbh_ruin_names (10 samples)\n",
      "Processing text outputs for task: leaderboard_bbh_reasoning_about_colored_objects (10 samples)\n",
      "Processing text outputs for task: leaderboard_bbh_penguins_in_a_table (10 samples)\n",
      "Processing text outputs for task: leaderboard_bbh_object_counting (10 samples)\n",
      "Processing text outputs for task: leaderboard_bbh_navigate (10 samples)\n",
      "Processing text outputs for task: leaderboard_bbh_movie_recommendation (10 samples)\n",
      "Processing text outputs for task: leaderboard_bbh_logical_deduction_three_objects (10 samples)\n",
      "Processing text outputs for task: leaderboard_bbh_logical_deduction_seven_objects (10 samples)\n",
      "Processing text outputs for task: leaderboard_bbh_logical_deduction_five_objects (10 samples)\n",
      "Processing text outputs for task: leaderboard_bbh_hyperbaton (10 samples)\n",
      "Processing text outputs for task: leaderboard_bbh_geometric_shapes (10 samples)\n",
      "Processing text outputs for task: leaderboard_bbh_formal_fallacies (10 samples)\n",
      "Processing text outputs for task: leaderboard_bbh_disambiguation_qa (10 samples)\n",
      "Processing text outputs for task: leaderboard_bbh_date_understanding (10 samples)\n",
      "Processing text outputs for task: leaderboard_bbh_causal_judgement (10 samples)\n",
      "Processing text outputs for task: leaderboard_bbh_boolean_expressions (10 samples)\n",
      "Results saved to /workspace/llm-evaluation/evaluations/Ministral-8B-Instruct-2410_2025-10-09_151651/results.json\n",
      "‚úÖ Professional report generated: /workspace/llm-evaluation/evaluations/Ministral-8B-Instruct-2410_2025-10-09_151651/report.md\n",
      "‚úÖ Task report generated: /workspace/llm-evaluation/evaluations/Ministral-8B-Instruct-2410_2025-10-09_151651/task_reports/leaderboard_math_precalculus_hard_report.md\n",
      "‚úÖ Task report generated: /workspace/llm-evaluation/evaluations/Ministral-8B-Instruct-2410_2025-10-09_151651/task_reports/leaderboard_math_prealgebra_hard_report.md\n",
      "‚úÖ Task report generated: /workspace/llm-evaluation/evaluations/Ministral-8B-Instruct-2410_2025-10-09_151651/task_reports/leaderboard_math_num_theory_hard_report.md\n",
      "‚úÖ Task report generated: /workspace/llm-evaluation/evaluations/Ministral-8B-Instruct-2410_2025-10-09_151651/task_reports/leaderboard_math_intermediate_algebra_hard_report.md\n",
      "‚úÖ Task report generated: /workspace/llm-evaluation/evaluations/Ministral-8B-Instruct-2410_2025-10-09_151651/task_reports/leaderboard_math_geometry_hard_report.md\n",
      "‚úÖ Task report generated: /workspace/llm-evaluation/evaluations/Ministral-8B-Instruct-2410_2025-10-09_151651/task_reports/leaderboard_math_counting_and_prob_hard_report.md\n",
      "‚úÖ Task report generated: /workspace/llm-evaluation/evaluations/Ministral-8B-Instruct-2410_2025-10-09_151651/task_reports/leaderboard_math_algebra_hard_report.md\n",
      "‚úÖ Task report generated: /workspace/llm-evaluation/evaluations/Ministral-8B-Instruct-2410_2025-10-09_151651/task_reports/leaderboard_ifeval_report.md\n",
      "‚úÖ Task report generated: /workspace/llm-evaluation/evaluations/Ministral-8B-Instruct-2410_2025-10-09_151651/task_reports/leaderboard_musr_team_allocation_report.md\n",
      "‚úÖ Task report generated: /workspace/llm-evaluation/evaluations/Ministral-8B-Instruct-2410_2025-10-09_151651/task_reports/leaderboard_musr_object_placements_report.md\n",
      "‚úÖ Task report generated: /workspace/llm-evaluation/evaluations/Ministral-8B-Instruct-2410_2025-10-09_151651/task_reports/leaderboard_musr_murder_mysteries_report.md\n",
      "‚úÖ Task report generated: /workspace/llm-evaluation/evaluations/Ministral-8B-Instruct-2410_2025-10-09_151651/task_reports/leaderboard_mmlu_pro_report.md\n",
      "‚úÖ Task report generated: /workspace/llm-evaluation/evaluations/Ministral-8B-Instruct-2410_2025-10-09_151651/task_reports/leaderboard_gpqa_extended_report.md\n",
      "‚úÖ Task report generated: /workspace/llm-evaluation/evaluations/Ministral-8B-Instruct-2410_2025-10-09_151651/task_reports/leaderboard_gpqa_diamond_report.md\n",
      "‚úÖ Task report generated: /workspace/llm-evaluation/evaluations/Ministral-8B-Instruct-2410_2025-10-09_151651/task_reports/leaderboard_gpqa_main_report.md\n",
      "‚úÖ Task report generated: /workspace/llm-evaluation/evaluations/Ministral-8B-Instruct-2410_2025-10-09_151651/task_reports/leaderboard_bbh_web_of_lies_report.md\n",
      "‚úÖ Task report generated: /workspace/llm-evaluation/evaluations/Ministral-8B-Instruct-2410_2025-10-09_151651/task_reports/leaderboard_bbh_tracking_shuffled_objects_three_objects_report.md\n",
      "‚úÖ Task report generated: /workspace/llm-evaluation/evaluations/Ministral-8B-Instruct-2410_2025-10-09_151651/task_reports/leaderboard_bbh_tracking_shuffled_objects_seven_objects_report.md\n",
      "‚úÖ Task report generated: /workspace/llm-evaluation/evaluations/Ministral-8B-Instruct-2410_2025-10-09_151651/task_reports/leaderboard_bbh_tracking_shuffled_objects_five_objects_report.md\n",
      "‚úÖ Task report generated: /workspace/llm-evaluation/evaluations/Ministral-8B-Instruct-2410_2025-10-09_151651/task_reports/leaderboard_bbh_temporal_sequences_report.md\n",
      "‚úÖ Task report generated: /workspace/llm-evaluation/evaluations/Ministral-8B-Instruct-2410_2025-10-09_151651/task_reports/leaderboard_bbh_sports_understanding_report.md\n",
      "‚úÖ Task report generated: /workspace/llm-evaluation/evaluations/Ministral-8B-Instruct-2410_2025-10-09_151651/task_reports/leaderboard_bbh_snarks_report.md\n",
      "‚úÖ Task report generated: /workspace/llm-evaluation/evaluations/Ministral-8B-Instruct-2410_2025-10-09_151651/task_reports/leaderboard_bbh_salient_translation_error_detection_report.md\n",
      "‚úÖ Task report generated: /workspace/llm-evaluation/evaluations/Ministral-8B-Instruct-2410_2025-10-09_151651/task_reports/leaderboard_bbh_ruin_names_report.md\n",
      "‚úÖ Task report generated: /workspace/llm-evaluation/evaluations/Ministral-8B-Instruct-2410_2025-10-09_151651/task_reports/leaderboard_bbh_reasoning_about_colored_objects_report.md\n",
      "‚úÖ Task report generated: /workspace/llm-evaluation/evaluations/Ministral-8B-Instruct-2410_2025-10-09_151651/task_reports/leaderboard_bbh_penguins_in_a_table_report.md\n",
      "‚úÖ Task report generated: /workspace/llm-evaluation/evaluations/Ministral-8B-Instruct-2410_2025-10-09_151651/task_reports/leaderboard_bbh_object_counting_report.md\n",
      "‚úÖ Task report generated: /workspace/llm-evaluation/evaluations/Ministral-8B-Instruct-2410_2025-10-09_151651/task_reports/leaderboard_bbh_navigate_report.md\n",
      "‚úÖ Task report generated: /workspace/llm-evaluation/evaluations/Ministral-8B-Instruct-2410_2025-10-09_151651/task_reports/leaderboard_bbh_movie_recommendation_report.md\n",
      "‚úÖ Task report generated: /workspace/llm-evaluation/evaluations/Ministral-8B-Instruct-2410_2025-10-09_151651/task_reports/leaderboard_bbh_logical_deduction_three_objects_report.md\n",
      "‚úÖ Task report generated: /workspace/llm-evaluation/evaluations/Ministral-8B-Instruct-2410_2025-10-09_151651/task_reports/leaderboard_bbh_logical_deduction_seven_objects_report.md\n",
      "‚úÖ Task report generated: /workspace/llm-evaluation/evaluations/Ministral-8B-Instruct-2410_2025-10-09_151651/task_reports/leaderboard_bbh_logical_deduction_five_objects_report.md\n",
      "‚úÖ Task report generated: /workspace/llm-evaluation/evaluations/Ministral-8B-Instruct-2410_2025-10-09_151651/task_reports/leaderboard_bbh_hyperbaton_report.md\n",
      "‚úÖ Task report generated: /workspace/llm-evaluation/evaluations/Ministral-8B-Instruct-2410_2025-10-09_151651/task_reports/leaderboard_bbh_geometric_shapes_report.md\n",
      "‚úÖ Task report generated: /workspace/llm-evaluation/evaluations/Ministral-8B-Instruct-2410_2025-10-09_151651/task_reports/leaderboard_bbh_formal_fallacies_report.md\n",
      "‚úÖ Task report generated: /workspace/llm-evaluation/evaluations/Ministral-8B-Instruct-2410_2025-10-09_151651/task_reports/leaderboard_bbh_disambiguation_qa_report.md\n",
      "‚úÖ Task report generated: /workspace/llm-evaluation/evaluations/Ministral-8B-Instruct-2410_2025-10-09_151651/task_reports/leaderboard_bbh_date_understanding_report.md\n",
      "‚úÖ Task report generated: /workspace/llm-evaluation/evaluations/Ministral-8B-Instruct-2410_2025-10-09_151651/task_reports/leaderboard_bbh_causal_judgement_report.md\n",
      "‚úÖ Task report generated: /workspace/llm-evaluation/evaluations/Ministral-8B-Instruct-2410_2025-10-09_151651/task_reports/leaderboard_bbh_boolean_expressions_report.md\n",
      "‚úÖ Generated 39 task-specific reports\n",
      "‚úÖ Professional report generated: /workspace/llm-evaluation/evaluations/Ministral-8B-Instruct-2410_2025-10-09_151651/report.md\n",
      "‚úÖ Organized evaluation saved to: /workspace/llm-evaluation/evaluations/Ministral-8B-Instruct-2410_2025-10-09_151651\n",
      "Clearing GPU memory...\n",
      "GPU memory cleared.\n"
     ]
    }
   ],
   "source": [
    "# Option 2: Specifying the task group\n",
    "!python llm_eval_cli.py \\\n",
    "  --model hf \\\n",
    "  --model_name mistralai/Ministral-8B-Instruct-2410 \\\n",
    "  --tasks LEADERBOARD \\\n",
    "  --device cuda \\\n",
    "  --num_samples 10 \\\n",
    "  --quantize\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## üìÇ File Structure Update\n",
    "\n",
    "**UPDATED**: The evaluation system has been reverted to the **classic file structure**:\n",
    "\n",
    "```\n",
    "llm-evaluation/\n",
    "‚îú‚îÄ‚îÄ results/                    # JSON evaluation results\n",
    "‚îÇ   ‚îî‚îÄ‚îÄ results_ModelName_*.json\n",
    "‚îú‚îÄ‚îÄ reports/                    # Markdown reports  \n",
    "‚îÇ   ‚îî‚îÄ‚îÄ results_ModelName_*_professional_report.md\n",
    "‚îî‚îÄ‚îÄ ...\n",
    "```\n",
    "\n",
    "This replaces the previous organized structure with separate model directories. The classic structure keeps results and reports in dedicated directories for backward compatibility.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Examining captured model text outputs...\n",
      "/workspace/llm-evaluation/results/results_Ministral-8B-Instruct-2410_hf_leaderboard_bbh_boolean_expressions_l..._20250710_122909.json\n",
      "Found samples section!\n",
      "\n",
      "=== Task: leaderboard_mmlu_pro (50 samples) ===\n",
      "\n",
      "üìù Sample 1:\n",
      "  üéØ Target Answer: I\n",
      "  ü§ñ Model Output: A\n",
      "  ‚úÖ Correct: False\n",
      "  ‚ùì Question: Typical advertising regulatory bodies suggest, for example that adverts must not: encourage _________, cause unnecessary ________ or _____, and must n...\n",
      "  üìä Response scores: [[[-2.703125, False]], [[-3.953125, False]], [[-3.703125, False]], [[-0.8359375, True]], [[-3.203125, False]], [[-2.453125, False]], [[-2.703125, False]], [[-2.578125, False]], [[-1.7109375, False]]]\n",
      "\n",
      "üìù Sample 2:\n",
      "  üéØ Target Answer: F\n",
      "  ü§ñ Model Output: A\n",
      "  ‚úÖ Correct: False\n",
      "  ‚ùì Question: Managers are entrusted to run the company in the best interest of ________. Specifically, they have a duty to act for the benefit of the company, as w...\n",
      "  üìä Response scores: [[[-3.34375, False]], [[-0.72265625, True]], [[-4.46875, False]], [[-2.71875, False]], [[-6.21875, False]], [[-1.21875, False]], [[-3.21875, False]], [[-4.21875, False]], [[-5.46875, False]], [[-3.09375, False]]]\n",
      "\n",
      "üìù Sample 3:\n",
      "  üéØ Target Answer: J\n",
      "  ü§ñ Model Output: A\n",
      "  ‚úÖ Correct: False\n",
      "  ‚ùì Question: There are two main issues associated with _____ sizing. _______ is a key issue as due to the information policy of the corporation it can be argued th...\n",
      "  üìä Response scores: [[[-1.9765625, False]], [[-2.71875, False]], [[-3.09375, False]], [[-2.46875, False]], [[-2.71875, False]], [[-1.4765625, True]], [[-2.09375, False]], [[-2.96875, False]], [[-1.9765625, False]], [[-2.84375, False]]]\n",
      "\n",
      "=== Task: leaderboard_bbh_boolean_expressions (50 samples) ===\n",
      "\n",
      "üìù Sample 1:\n",
      "  üéØ Target Answer: False\n",
      "  ü§ñ Model Output: False\n",
      "  ‚úÖ Correct: True\n",
      "  ‚ùì Question: not ( True ) and ( True ) is\n",
      "  üìä Response scores: [[[-0.169921875, True]], [[-1.921875, False]]]\n",
      "\n",
      "üìù Sample 2:\n",
      "  üéØ Target Answer: True\n",
      "  ü§ñ Model Output: False\n",
      "  ‚úÖ Correct: False\n",
      "  ‚ùì Question: True and not not ( not False ) is\n",
      "  üìä Response scores: [[[-1.609375, False]], [[-0.2333984375, True]]]\n",
      "\n",
      "üìù Sample 3:\n",
      "  üéØ Target Answer: False\n",
      "  ü§ñ Model Output: False\n",
      "  ‚úÖ Correct: True\n",
      "  ‚ùì Question: not True or False or ( False ) is\n",
      "  üìä Response scores: [[[-1.1484375, False]], [[-0.3984375, True]]]\n",
      "\n",
      "... (showing first 6 samples with text outputs)\n",
      "\n",
      "üéâ SUCCESS! Total samples with text outputs: 6\n",
      "\n",
      "‚úÖ Model text outputs are now captured in the results file!\n",
      "‚úÖ Each sample includes:\n",
      "   - model_output: The actual decoded text response from the model\n",
      "   - output_matches_target: Boolean indicating if the response matches the target\n",
      "   - target: The expected correct answer\n",
      "   - doc: The original question/prompt\n",
      "\n",
      "üìä These can be used for detailed analysis and comparison with target answers.\n",
      "\n",
      "üìÑ Professional report generated at: /workspace/llm-evaluation/reports/results_Ministral-8B-Instruct-2410_hf_leaderboard_bbh_boolean_expressions_l..._20250710_122909_professional_report.md\n",
      "‚úÖ Report includes enhanced Model Output Analysis section!\n",
      "‚úÖ Report includes text output statistics and visualizations!\n"
     ]
    }
   ],
   "source": [
    "# ‚úÖ NEW: Demonstrate how to access captured model text outputs\n",
    "import json\n",
    "\n",
    "if output_path and os.path.exists(output_path):\n",
    "    print(\"üîç Examining captured model text outputs...\")\n",
    "    print(output_path)\n",
    "    \n",
    "    with open(output_path, 'r') as f:\n",
    "        results_data = json.load(f)\n",
    "    \n",
    "    # ‚úÖ FIXED: Check the correct location for samples (top-level \"samples\" field)\n",
    "    sample_count = 0\n",
    "    if \"samples\" in results_data:\n",
    "        print(\"Found samples section!\")\n",
    "        \n",
    "        for task_name, task_samples in results_data[\"samples\"].items():\n",
    "            if isinstance(task_samples, list) and len(task_samples) > 0:\n",
    "                print(f\"\\n=== Task: {task_name} ({len(task_samples)} samples) ===\")\n",
    "                \n",
    "                for i, sample in enumerate(task_samples[:3]):  # Show first 3 samples\n",
    "                    if isinstance(sample, dict) and \"model_output\" in sample:\n",
    "                        sample_count += 1\n",
    "                        print(f\"\\nüìù Sample {i+1}:\")\n",
    "                        print(f\"  üéØ Target Answer: {sample.get('target', 'N/A')}\")\n",
    "                        print(f\"  ü§ñ Model Output: {sample.get('model_output', 'N/A')}\")\n",
    "                        print(f\"  ‚úÖ Correct: {sample.get('output_matches_target', 'N/A')}\")\n",
    "                        \n",
    "                        # Show the question/prompt if available\n",
    "                        if \"doc\" in sample and isinstance(sample[\"doc\"], dict):\n",
    "                            # Different tasks have different field names for questions\n",
    "                            question_field = None\n",
    "                            if \"input\" in sample[\"doc\"]:\n",
    "                                question_field = \"input\"\n",
    "                            elif \"question\" in sample[\"doc\"]:\n",
    "                                question_field = \"question\"\n",
    "                            elif \"narrative\" in sample[\"doc\"]:\n",
    "                                question_field = \"narrative\"\n",
    "                            \n",
    "                            if question_field:\n",
    "                                question = sample[\"doc\"][question_field]\n",
    "                                # Truncate long questions\n",
    "                                if len(question) > 150:\n",
    "                                    question = question[:150] + \"...\"\n",
    "                                print(f\"  ‚ùì Question: {question}\")\n",
    "                        \n",
    "                        # Show response details for debugging\n",
    "                        if \"resps\" in sample and len(sample[\"resps\"]) > 0:\n",
    "                            print(f\"  üìä Response scores: {sample['resps']}\")\n",
    "                        \n",
    "                if sample_count >= 6:  # Limit to avoid too much output\n",
    "                    print(f\"\\n... (showing first {sample_count} samples with text outputs)\")\n",
    "                    break\n",
    "        \n",
    "        if sample_count == 0:\n",
    "            print(\"‚ùå No samples with text outputs found.\")\n",
    "        else:\n",
    "            print(f\"\\nüéâ SUCCESS! Total samples with text outputs: {sample_count}\")\n",
    "            print(\"\\n‚úÖ Model text outputs are now captured in the results file!\")\n",
    "            print(\"‚úÖ Each sample includes:\")\n",
    "            print(\"   - model_output: The actual decoded text response from the model\")\n",
    "            print(\"   - output_matches_target: Boolean indicating if the response matches the target\")\n",
    "            print(\"   - target: The expected correct answer\")\n",
    "            print(\"   - doc: The original question/prompt\")\n",
    "            print(\"\\nüìä These can be used for detailed analysis and comparison with target answers.\")\n",
    "    else:\n",
    "        print(\"‚ùå No 'samples' section found in results.\")\n",
    "else:\n",
    "    print(\"‚ùå Results file not found - cannot examine text outputs.\")\n",
    "\n",
    "# Check if the report includes model outputs\n",
    "if output_path:\n",
    "    # Get the base filename without extension\n",
    "    basename = os.path.basename(output_path)\n",
    "    basename = os.path.splitext(basename)[0]\n",
    "\n",
    "    # Construct the report path\n",
    "    reports_dir = get_reports_dir()\n",
    "    report_path = os.path.join(reports_dir, f\"{basename}_professional_report.md\")\n",
    "\n",
    "    if os.path.exists(report_path):\n",
    "        print(f\"\\nüìÑ Professional report generated at: {report_path}\")\n",
    "        \n",
    "        # Check if the report contains model output information\n",
    "        with open(report_path, 'r') as f:\n",
    "            report_content = f.read()\n",
    "        \n",
    "        if \"Model Output Analysis\" in report_content:\n",
    "            print(\"‚úÖ Report includes enhanced Model Output Analysis section!\")\n",
    "        if \"model_output\" in report_content.lower():\n",
    "            print(\"‚úÖ Report includes captured model text outputs!\")\n",
    "        if \"Text Output Summary\" in report_content:\n",
    "            print(\"‚úÖ Report includes text output statistics and visualizations!\")\n",
    "    else:\n",
    "        print(f\"‚ùå Report not found at: {report_path}\")\n",
    "else:\n",
    "    print(\"‚ùå No output path available. Cannot check for report.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Evaluation (.evaluation)",
   "language": "python",
   "name": "evaluation"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
